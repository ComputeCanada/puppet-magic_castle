---
lookup_options:
  profile::users::ldap::users:
    merge: 'deep'
  profile::users::ldap::groups:
    merge: 'deep'
  profile::users::local::users:
    merge: 'deep'
  jupyterhub::jupyterhub_config_hash:
    merge: 'deep'
  prometheus::alerts:
    merge: 'deep'
  prometheus::extra_alert:
    merge: 'hash'
  profile::volumes::devices:
    merge: 'deep'

profile::base::version: 14.3.0
profile::base::packages: []

motd::content: ""

consul_template::version: 0.25.2
consul::version: 1.15.2
consul_template::config_hash:
  consul:
    token: "%{hiera('profile::consul::acl_api_token')}"

epel::epel_exclude: 'slurm*'
epel::epel_source_managed: false
epel::epel_debuginfo_managed: false
epel::epel_testing_managed: false
epel::epel_testing_source_managed: false
epel::epel_testing_debuginfo_managed: false

fail2ban::package_name: fail2ban-server
fail2ban::jails: ['ssh-route', 'ssh-ban-root']
fail2ban::custom_jails:
  'ssh-route':
    enabled: true
    filter: 'sshd'
    findtime: 3600
    bantime: 86400
    maxretry: 20
    action: 'route'
    logpath: '%(sshd_log)s'
  'ssh-ban-root':
    enabled: true
    findtime: 3600
    bantime: 86400
    maxretry: 0
    action: 'route'
    logpath: '%(sshd_log)s'
    journalmatch: '_SYSTEMD_UNIT=sshd.service + _COMM=sshd'
    filter_maxlines: 10
    filter_includes: 'before = common.conf'
    filter_failregex: '^%(__prefix_line)spam_unix\(sshd:auth\):\s+authentication failure;\s*logname=\S*\s*uid=\d*\s*euid=\d*\s*tty=\S*\s*ruser=\S*\s*rhost=<HOST>\S*\s*user=(root|admin)\s.*$'

jupyterhub::kernel::install_method: venv
jupyterhub::jupyterhub_config_hash:
  SlurmFormSpawner:
    ui_args:
      notebook:
        name: Jupyter Notebook
        url: '/tree'
      lab:
        name: JupyterLab
      terminal:
        name: Terminal
        url: '/terminals/1'
      rstudio:
        name: RStudio
        url: '/rstudio'
      code-server:
        name: VS Code
        url: '/code-server'
      desktop:
        name: Desktop
        url: '/Desktop'

  SbatchForm:
    ui:
      choices: ['notebook', 'lab', 'terminal', 'code-server', 'desktop']
      def: 'lab'

selinux::mode: 'permissive'
# selinux::type: 'targeted'

squid::cache_mem: "256 MB"
squid::extra_config_sections:
  general:
    config_entries:
      maximum_object_size: "131072 KB"

swap_file::files:
  default:
    ensure: "present"
    swapfile: "/mnt/swap"
    swapfilesize: "1 GB"


mysql::server::remove_default_accounts: true
mysql::server::override_options:
    mysqld:
      innodb_buffer_pool_size: 1024M
      innodb_log_file_size: 64M
      innodb_lock_wait_timeout: 900

prometheus::alerts:
  groups:
    - name: slurm
      rules:
        - alert: Slurm-nodes-down
          expr: slurm_nodes_down  > 0
          labels:
            severity: critical
          annotations:
            summary: "Slurm reports down nodes"
            description: "slurmctld on {{ $labels.instance }} reports {{ $value }} nodes in the DOWN state"
    - name: filesystem
      rules:
        - record: node_fileystem_avail_ratio
          expr: node_filesystem_avail_bytes{device=~"/dev/.*"} / node_filesystem_size_bytes{device=~"/dev/.*"}
        - alert: Filesystem-low-on-avail-space
          expr: node_fileystem_avail_ratio * 100 < 10.0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Instance {{ $labels.instance }}'s {{ $labels.device }} has less than 10% space available"
            description: "{{ $labels.instance }}'s {{ $labels.device }}(mountpoint={{ $labels.mountpoint }}) has {{ printf \"%.2f\" $value }}% space available"
        - alert: Filesystem-low-on-avail-space
          expr: node_fileystem_avail_ratio * 100 < 5.0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "Instance {{ $labels.instance }}'s {{ $labels.device }} has less than 5% space available"
            description: "{{ $labels.instance }}'s {{ $labels.device }}(mountpoint={{ $labels.mountpoint }}) has {{ printf \"%.2f\" $value }}% space available"
    - name: puppet
      rules:
        - alert: Puppet-Failure
          expr: puppet_report_events{name="Failure"} > 0
          for: 1m
          labels:
            severity: error
          annotations:
            summary: "Instance {{ $labels.host }} reported puppet failure"
            description: "{{ $labels.host }} reported {{ $value }} errors"

# Prometheus record and alert rules are put in the same type of files
# that puppet-prometheus classified as "alerts".
prometheus::extra_alerts:
  slurm_job:
    groups:
      - name: 'slurm_job_core'
        rules:
        - record: slurm_job:allocated_core:count
          expr: count(slurm_job_core_usage_total)
        - record: slurm_job:allocated_core:count_user_account
          expr: count(slurm_job_core_usage_total) by (user,account)
        - record: slurm_job:used_core:sum
          expr: sum(rate(slurm_job_core_usage_total{}[2m]) / 1000000000)
        - record: slurm_job:used_core:sum_user_account
          expr: sum(rate(slurm_job_core_usage_total{}[2m]) / 1000000000) by (user,account)
      - name: 'slurm_job_memory'
        rules:
        - record: slurm_job:allocated_memory:sum
          expr: sum(slurm_job_memory_limit{})
        - record: slurm_job:allocated_memory:sum_user_account
          expr: sum(slurm_job_memory_limit{}) by (user,account)
        - record: slurm_job:rss_memory:sum
          expr: sum(slurm_job_memory_rss)
        - record: slurm_job:rss_memory:sum_user_account
          expr: sum(slurm_job_memory_rss) by (user, account)
        - record: slurm_job:max_memory:sum_user_account
          expr: sum(slurm_job_memory_max) by (user, account)
      - name: 'slurm_job_gpu'
        rules:
        - record: slurm_job:allocated_gpu:count
          expr: count(slurm_job_utilization_gpu)
        - record: slurm_job:allocated_gpu:count_user_account
          expr: count(slurm_job_utilization_gpu) by (user, account)
        - record: slurm_job:used_gpu:sum
          expr: sum(slurm_job_utilization_gpu) / 100
        - record: slurm_job:used_gpu:sum_user_account
          expr: sum(slurm_job_utilization_gpu) by (user,account) / 100
        - record: slurm_job:non_idle_gpu:sum_user_account
          expr: count(slurm_job_utilization_gpu > 0) by (user,account)
        - record: slurm_job:power_gpu:sum
          expr: sum(slurm_job_power_gpu)
        - record: slurm_job:power_gpu:sum_user_account
          expr: sum(slurm_job_power_gpu) by (user,account)

prometheus::node_exporter::version: 1.5.0
prometheus::node_exporter::collectors_enable:
  - "textfile.directory=/var/lib/node_exporter"
prometheus::server::version: 2.39.0
prometheus::server::scrape_configs:
  - job_name: exporter
    scrape_interval: 10s
    scrape_timeout: 10s
    honor_labels: true
    consul_sd_configs:
      - server: 127.0.0.1:8500
        token: "%{hiera('profile::consul::acl_api_token')}"
    relabel_configs:
      - source_labels:
          - __meta_consul_tags
        regex: '.*,exporter,.*'
        action: keep
      - source_labels:
          - __meta_consul_node
        target_label: instance
      - source_labels:
          - __meta_consul_service
        target_label: job
  - job_name: jupyterhub
    metrics_path: "/hub/metrics"
    scrape_interval: 10s
    scrape_timeout: 10s
    honor_labels: true
    authorization:
      type: Bearer
      credentials: "%{hiera('jupyterhub::prometheus_token')}"
    consul_sd_configs:
      - server: 127.0.0.1:8500
        token: "%{hiera('profile::consul::acl_api_token')}"
    relabel_configs:
      - source_labels:
          - __meta_consul_tags
        regex: '.*,jupyterhub,.*'
        action: keep
      - source_labels:
          - __meta_consul_node
        target_label: instance

prometheus::storage_retention: '48h'
prometheus::storage_retention_size: '5GB'

prometheus::alertmanager::version: '0.26.0'
# The default value has a syntax issue in the original puppet-prometheus
# https://github.com/voxpupuli/puppet-prometheus/pull/540
prometheus::alertmanager::receivers:
  - name: 'Admin'
    email_configs:
      - to: 'root@localhost'

prometheus::alertmanagers_config:
  - static_configs:
    - targets:
      - "%{hiera('terraform.tag_ip.mgmt.0')}:9093"

prometheus::alertmanager::inhibit_rules:
  - source_matchers: [ severity = critical ]
    target_matchers: [ severity = warning ]
    equal: ['instance', 'alertname']

prometheus::external_url: "https://metrics.%{lookup('terraform.data.domain_name')}/"
prometheus::alertmanager::extra_options: "--web.external-url=%{lookup('prometheus::external_url')}"

profile::squid::server::port: 3128
profile::squid::server::cache_size: 4096
profile::squid::server::cvmfs_acl_regex:
  - '^(cvmfs-.*\.computecanada\.ca)$'
  - '^(cvmfs-.*\.computecanada\.net)$'
  - '^(object-.*\.cloud\.computecanada\.ca)$'
  - '^(.*-cvmfs\.openhtc\.io)$'
  - '^(cvmfs-.*\.genap\.ca)$'
  - '^(.*s1\.eessi\.science)$'

profile::cvmfs::client::disable_autofs: "%{facts.is_unprivileged_container}"
profile::cvmfs::client::quota_limit: 4096
profile::cvmfs::client::repositories:
  - software.eessi.io
  - cvmfs-config.computecanada.ca
  - soft.computecanada.ca
profile::cvmfs::local_user::group: 'cvmfs-reserved'
profile::cvmfs::local_user::uid: 13000004
profile::cvmfs::local_user::gid: 8000131

profile::freeipa::mokey::port: 12345
profile::freeipa::mokey::enable_user_signup: true
profile::freeipa::mokey::require_verify_admin: true

profile::freeipa::server::id_start: 60001
profile::software_stack::min_uid: "%{alias('profile::freeipa::server::id_start')}"

profile::slurm::base::slurm_version: '24.11'
profile::slurm::base::os_reserved_memory: 512
profile::slurm::controller::autoscale_version: '0.7.0'
profile::slurm::node::enable_tmpfs_mounts: true

profile::accounts::project_regex: '(ctb|def|rpp|rrg)-[a-z0-9_-]*'
profile::users::ldap::users:
  'user':
    count: "%{alias('terraform.data.nb_users')}"
    passwd: "%{alias('terraform.data.guest_passwd')}"
    manage_password: true

profile::users::ldap::groups:
  'def-sponsor00':
    automember: true
    hbac_rules: ['login:sshd', 'node:sshd', 'proxy:jupyterhub-login']

profile::users::local::users:
  "%{alias('terraform.data.sudoer_username')}":
    public_keys: "%{alias('terraform.data.public_keys')}"
    groups: ['adm', 'wheel', 'systemd-journal']
    sudoer: true
    authenticationmethods: 'publickey'


profile::freeipa::base::ipa_domain: "int.%{lookup('terraform.data.domain_name')}"

profile::slurm::base::cluster_name: "%{alias('terraform.data.cluster_name')}"

profile::freeipa::client::server_ip: "%{alias('terraform.tag_ip.mgmt.0')}"
profile::consul::servers: "%{alias('terraform.tag_ip.puppet')}"

profile::nfs::domain: "%{lookup('profile::freeipa::base::ipa_domain')}"
profile::nfs::client::server_ip: "%{alias('terraform.tag_ip.nfs.0')}"
profile::volumes::devices: "%{alias('terraform.self.volumes')}"

profile::mail::base::origin: "%{alias('terraform.data.domain_name')}"
profile::mail::sender::relayhosts: "%{alias('terraform.tag_ip.public')}"

profile::reverse_proxy::domain_name: "%{alias('terraform.data.domain_name')}"
profile::reverse_proxy::subdomains:
  ipa: "ipa.%{lookup('profile::freeipa::base::ipa_domain')}"
  mokey: "%{lookup('terraform.tag_ip.mgmt.0')}:%{lookup('profile::freeipa::mokey::port')}"
  jupyter: "https://127.0.0.1:8000"
  explore: "ipa.int.%{lookup('terraform.data.domain_name')}:9000"

profile::jupyterhub::hub::register_url: "https://mokey.%{lookup('terraform.data.domain_name')}/auth/signup"
profile::jupyterhub::hub::reset_pw_url: "https://mokey.%{lookup('terraform.data.domain_name')}/auth/forgotpw"

profile::gpu::restrict_profiling: false
profile::gpu::install::passthrough::packages:
  - nvidia-driver-cuda-libs
  - nvidia-driver
  - nvidia-driver-devel
  - nvidia-driver-libs
  - nvidia-driver-NVML
  - nvidia-modprobe
  - nvidia-xconfig
  - nvidia-persistenced
  - nvidia-driver-cuda

profile::userportal::server::prometheus_ip: "%{alias('terraform.tag_ip.mgmt.0')}"
profile::userportal::server::prometheus_port: 9090
profile::userportal::server::db_ip: 127.0.0.1
profile::userportal::server::db_port: 3306
profile::userportal::slurm_jobscripts::token: "%{alias('profile::userportal::server::root_api_token')}"